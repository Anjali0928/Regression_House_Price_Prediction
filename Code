# ─── 1) Imports ────────────────────────────────────────────────────────────────
import pandas as pd #for easy DataFrame handling
from sklearn.datasets import fetch_california_housing #loads the housing data
from sklearn.model_selection import train_test_split #to split data into train/test sets
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score #compute evaluation metrics

# ─── 2) Load the California Housing dataset ───────────────────────────────────
housing = fetch_california_housing(as_frame=True) #returns a pandas DataFrame with all data
df = housing.frame  # pandas DataFrame with 20640 rows and 9 columns (8 features + target)

# Display first 5 rows to see what we have
print(df.head())


'''
MedInc – Median income in the district (in 10,000s of dollars).
HouseAge – Median age of houses in the district.
AveRooms – Average number of rooms per household.
AveBedrms – Average number of bedrooms per household.
Population – Total population of the district.
AveOccup – Average number of household members.
Latitude – Latitude of the district (geographic location).
Longitude – Longitude of the district (geographic location).

Target (Dependent Variable)
MedHouseVal – Median value of houses (in $100,000s).
'''

# ─── 3) Prepare feature matrix X and target vector y ──────────────────────────
X = df.drop(columns=['MedHouseVal'])  # all columns except 'MedHouseVal'
y = df['MedHouseVal']                 # the column we want to predict
'''
X: DataFrame of shape (20640, 8) with the predictor variables.
y: Series of length 20640 with the values we want to predict.
'''

# ─── 4) Split into training and test sets ────────────────────────────────────
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.20,      # 20% of data goes to X_test, y_test
    random_state=0      # fixed seed so the split is reproducible
)
'''
20% of data reserved for testing (test_size=0.2).
random_state=0 ensures you get the same split every time you run.
'''

# ─── 5) Train a plain Linear Regression model ────────────────────────────────
lin_reg = LinearRegression()      #  Instantiate a plain linear model
lin_reg.fit(X_train, y_train)     #  learn a set of weights from the training data by minimizing squared error on training data.

y_pred_lin = lin_reg.predict(X_test)  # predict on the test set


# ─── 6) Train a Ridge Regression model (L2 regularization) ──────────────────
ridge_reg = Ridge(alpha=1.0)      #  alpha=1.0 controls strength of penalty
ridge_reg.fit(X_train, y_train)
y_pred_ridge = ridge_reg.predict(X_test)

# ─── 7) Train a Lasso Regression model (L1 regularization) ──────────────────
lasso_reg = Lasso(alpha=0.01)      #  alpha=0.1 often a good starting point
lasso_reg.fit(X_train, y_train)
y_pred_lasso = lasso_reg.predict(X_test)

# ─── 8) Evaluate all three models ────────────────────────────────────────────
for name, y_pred in [
    ("Linear Regression",    y_pred_lin),
    ("Ridge Regression",     y_pred_ridge),
    ("Lasso Regression",     y_pred_lasso),
]:

    mse  = mean_squared_error(y_test, y_pred)  # average squared difference
    rmse = mse ** 0.5                          # root of MSE for same units as target
    r2   = r2_score(y_test, y_pred)
    print(f"{name:20s} -> RMSE: {rmse:.3f}, R²: {r2:.3f}")

'''
What to Observe:
Which model has the lowest RMSE?
Which model has the highest R²?
Does regularization (Ridge/Lasso) help compared to plain Linear Regression?
Does Lasso zero out any coefficients? If you inspect lasso_reg.coef_, you’ll see some features might get weight = 0.



This end‑to‑end example shows you how to:
Load a real dataset (California Housing)
Prepare data for supervised learning
Train and compare Linear, Ridge, and Lasso regression
Evaluate with RMSE and R²

'''


Linear Regression    -> RMSE: 0.727, R²: 0.594
Ridge Regression     -> RMSE: 0.727, R²: 0.594
Lasso Regression     -> RMSE: 0.787, R²: 0.525

